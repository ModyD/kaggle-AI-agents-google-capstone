---
agent: agent
model: Claude Opus 4.5 (Preview) (copilot)
---
# ------------------------------
# 00 - Project context (paste this once at top of Copilot chat)
# ------------------------------
Context: We're building the backend for "Enterprise Security Incident Triage & Autonomous Runbook Agent".
Tech stack: Python 3.11, FastAPI(+OpenAPI), Pydantic v2, LangChain (Vertex/Gemini), psycopg2/asyncpg + pgvector (Neon), redis-py (Upstash), Docker, Google Cloud Run, google-adk, google-genai, uv for backend python (uv+venv for packages, no global installs) and others listed in backend/requirements.txt .
Purpose: Provide OpenAPI tools for the Vertex Agent Builder and frontend. Agents: triage (heuristic), explain (Gemini via LangChain), runbook (Gemini + RAG), safety (policy_check), simulate (simulator), orchestrated via A2A JSON messages. All LLM outputs must be validated using Pydantic models.
Generate one file per prompt below.

Project directory structure (for entire repo):
/ (repo root)
├─ .gitignore
├─ .env.template                # template for all env vars (NO secrets committed)
├─ .env                        # (local only — NOT committed; listed in .gitignore)
├─ README.md
├─ pyproject.toml              # optional repo-level metadata (if needed) (not required)
├─ uv.lock                     # optional repo-level UV lock (if used globally) (not required)
├─ .github/
│  └─ workflows/
│     └─ ci-backend.yml
│
├─ frontend/                    # Next.js + Tailwind + shadcn (ignore for now lets focus on backend)
│  ├─ package.json
│  ├─ tsconfig.json
│  ├─ app/
│  ├─ components/
│  └─ vercel.json
│
├─ backend/                     # FastAPI backend (uv / pyproject inside)
│  ├─ pyproject.toml            # generated by `uv init` for backend deps
│  ├─ uv.lock
│  ├─ .env.template             # backend-specific template (optional; mirrors root)
│  ├─ Dockerfile
│  ├─ cloudrun_deploy.sh
│  ├─ README.md                 # backend-specific docs
│  ├─ app/
│  │  ├─ __init__.py
│  │  ├─ config.py
│  │  ├─ models.py
│  │  ├─ triage.py
│  │  ├─ explain.py
│  │  ├─ runbook.py
│  │  ├─ rag.py
│  │  ├─ policy.py
│  │  ├─ simulate.py
│  │  ├─ a2a.py
│  │  ├─ db.py
│  │  └─ observability.py
│  ├─ api/
│  │  ├─ __init__.py
│  │  ├─ main.py
│  │  ├─ routes_triage.py
│  │  ├─ routes_explain.py
│  │  ├─ routes_runbook.py
│  │  ├─ routes_policy.py
│  │  ├─ routes_simulate.py
│  │  ├─ routes_flow.py
│  │  ├─ routes_health.py
│  │  └─ routes_memory.py
│  └─ tests/
│     ├─ test_triage.py
│     ├─ test_policy.py
│     ├─ test_a2a_flow.py
│     └─ test_runbook_stub.py
│
├─ infra/
│  ├─ terraform/ (optional)
│  └─ cloudrun_deploy.sh
│
├─ notebooks/
│  ├─ submission_notebook.ipynb
│  └─ data_examples.ipynb
│
└─ docs/
   ├─ architecture.png
   └─ design_notes.md

# ------------------------------
# 01 - models.py (Pydantic schema file)
# ------------------------------
Prompt:
"Generate a Python file `backend/app/models.py` using Pydantic v2. Create typed models for:
- IncidentRequest (incident_id: Optional[str], features: dict[str, Any])
- TriageResult (label: Literal['LOW','MEDIUM','HIGH'], score: int, contribs: list[tuple[str,int]])
- RunbookStep (step: str, why: str, risk: Literal['low','medium','high'])
- RunbookResponse (runbook: list[RunbookStep], source: str)
- A2AMessage (id, from_agent, to_agent, type, timestamp, payload: dict, trace_id)
- TimelineEntry (actor: str, type: str, payload: dict, trace_id: str, timestamp: datetime)
Ensure Pydantic validators for types and helpful docstrings. Use typing imports and ISO datetime parsing."

# ------------------------------
# 02 - triage.py (heuristic rule engine)
# ------------------------------
Prompt:
"Generate `backend/app/triage.py`. Implement a deterministic, explainable rule-based triage engine:
- Function: score_incident(features: dict) -> Tuple[str,int,list[tuple[str,int]]]
- Use weighted rules (failed_logins_last_hour, process_spawn_count, suspicious_file_activity, rare_outgoing_connection). Provide clear constants for weights.
- Return label mapping: score >=6 => HIGH, >=3 => MEDIUM, else LOW.
- Add a small helper to normalize features and a unit-testable seed of example features."

# ------------------------------
# 03 - a2a.py (orchestrator with async flow)
# ------------------------------
Prompt:
"Generate `backend/app/a2a.py`. Implement an async orchestrator function `async def orchestrate_flow(incident_id: str | None, features: dict) -> list[TimelineEntry]` that:
- uses triage.score_incident synchronously,
- concurrently calls explain.explain_incident(...) and runbook.generate_runbook(...) using asyncio.gather,
- calls policy.policy_check(...) to sanitize runbook,
- loops over sanitized steps and simulates execution with asyncio.sleep delays,
- creates TimelineEntry objects (use models.TimelineEntry) for each significant action and returns the ordered list.
- Add trace_id generation (uuid) and include timestamps."

# ------------------------------
# 04 - explain.py (LangChain + Gemini wrapper)
# ------------------------------
Prompt:
"Generate `backend/app/explain.py`. Create a function `async def explain_incident(features: dict, label: str, score: int, contribs: list) -> dict` that:
- uses LangChain prompt templates to craft a compact explanation prompt
- calls Vertex Gemini via LangChain's Vertex integration (show pseudocode or actual LangChain Vertex usage)
- requests JSON output with fields 'explanation' and 'reasons' (2 short bullets)
- validates the parsed JSON using Pydantic model TriageExplanation (create inline if not present)
- includes retry logic for malformed JSON with one retry prompt instructing Gemini to strictly output valid JSON
- returns a dict {'explanation':str,'reasons':[str,str]}"

# ------------------------------
# 05 - runbook.py (RAG + Gemini)
# ------------------------------
Prompt:
"Generate `backend/app/runbook.py`. Implement `async def generate_runbook(features: dict, label: str, score: int, contribs: list) -> RunbookResponse` which:
- retrieves k nearest runbooks from Neon pgvector using a helper function get_similar_runbooks(query_text, k=5),
- constructs a RAG prompt combining the incident explanation (brief) and the retrieved runbook texts,
- calls Gemini via LangChain to generate JSON with 'runbook' as an array of objects conforming to RunbookStep,
- validates output with Pydantic RunbookResponse,
- returns the validated RunbookResponse,
- include comments showing where to call Vertex Embedding API for the retrieval step (embed query and query pgvector)."

# ------------------------------
# 06 - rag.py (pgvector helpers)
# ------------------------------
Prompt:
"Generate `backend/app/rag.py`. Implement:
- async function embed_text(text: str) -> list[float] that calls Vertex Embeddings (provide placeholder using google-cloud-aiplatform or vertexai),
- async function get_similar_runbooks(conn, text: str, k: int=5) that:
  - computes embedding,
  - queries Neon pgvector using asyncpg or sqlalchemy for nearest neighbors ORDER BY embedding <-> query LIMIT k,
  - returns list of dicts {id, text, score}.
Include SQL snippet example for pgvector nearest neighbor query. Add error handling and comments about storing embeddings."

# ------------------------------
# 07 - policy.py (safety verifier)
# ------------------------------
Prompt:
"Generate `backend/app/policy.py`. Implement `def policy_check(runbook: RunbookResponse) -> dict` that:
- defines a FORBIDDEN list of substrings and regexes (e.g., 'rm -rf', 'shutdown', 'curl http', 'mkfs', 'dd if='),
- scans each runbook step; if forbidden content found, rewrites to a safe investigative action and records the change,
- returns dict with keys 'runbook' (safe steps) and 'changes' (list of {'from','to'}).
- Also include a function policy_is_safe(text: str) -> bool for quick checks."

# ------------------------------
# 08 - simulate.py (simulator)
# ------------------------------
Prompt:
"Generate `backend/app/simulate.py`. Implement `async def simulate_runbook(runbook: dict) -> list[dict]` that:
- iterates steps, for each step emits a simulated start and end result with timestamps,
- artificially marks some steps as 'simulated_ok' or 'simulated_warn' depending on step.risk,
- returns an ordered list of simulation events suitable for timeline entries."

# ------------------------------
# 09 - db.py (Neon + Redis clients)
# ------------------------------
Prompt:
"Generate `backend/app/db.py`. Implement async connection helpers:
- async def get_pg_conn() -> asyncpg.Connection (using NEON_DATABASE_URL env var),
- async def insert_runbook(id: str, text: str, embedding: list[float]) saving into runbooks(id, text, embedding),
- async def query_similar_runbooks(vector: list[float], k: int) -> list[dict],
- redis client helper get_redis() using upstash REST or redis-py with env vars,
- include simple caching helper cache_set/get for sessions and telemetry.
Add clear comments about env variables and recommended connection pooling."

# ------------------------------
# 10 - main.py (FastAPI app + routes)
# ------------------------------
Prompt:
"Generate `backend/app/main.py`. Create a FastAPI app that:
- mounts routes: POST /triage (returns TriageResult), POST /simulate_flow (runs orchestrate_flow),
- includes startup event to initialize DB and Redis clients and shutdown event to close them,
- uses logging with structlog or standard logging to output structured JSON logs,
- includes CORS middleware to allow the frontend origin (use env var FRONTEND_URL),
- includes healthcheck endpoint /health returning status and version,
- ensure routes return Pydantic models and proper response codes."

# ------------------------------
# 11 - Dockerfile (Python container)
# ------------------------------
Prompt:
"Generate `backend/Dockerfile` optimized for Cloud Run:
- Use python:3.11-slim base,
- install build deps, install dependencies from requirements.txt or pyproject,
- copy app into /app,
- set ENV PORT=8080 and entrypoint uvicorn app.main:app --host 0.0.0.0 --port $PORT,
- include non-root user creation and small image best practices."

# ------------------------------
# 12 - cloudrun_deploy.sh (deploy script)
# ------------------------------
Prompt:
"Generate `infra/cloudrun_deploy.sh` shell script that:
- builds container with Google Cloud Build or docker build & pushes to GCR,
- deploys to Cloud Run with `gcloud run deploy` using SERVICE_NAME, PROJECT_ID, REGION env vars,
- passes environment variables securely (or instruct to use Secret Manager),
- includes a post-deploy curl smoke-test to /health and /simulate_flow with a sample payload,
- prints the Cloud Run URL on success."

# ------------------------------
# 13 - tests (pytest) for triage & policy
# ------------------------------
Prompt:
"Generate `backend/app/tests/test_trige_policy.py` with pytest tests covering:
- triage.score_incident returns HIGH for features with high failed_logins/process spawns,
- triage returns LOW for innocuous features,
- policy.policy_check rewrites forbidden steps and records change,
- simulate.simulate_runbook returns events equal to number of steps * 2 (start/end).
Include pytest fixtures for sample runbook and sample features."

# ------------------------------
# 14 - logging & observability hooks
# ------------------------------
Prompt:
"Generate a small module `backend/app/observability.py` that:
- configures structured JSON logging for FastAPI using python's logging or structlog,
- exposes helper log_event(name: str, payload: dict) that logs with timestamp and trace_id,
- basic metrics stub functions increment_metric(name: str). Add comments on integrating Cloud Monitoring and OpenTelemetry."

# ------------------------------
# 15 - README backend section (for repo)
# ------------------------------
Prompt:
"Generate markdown for `backend/README.md` describing:
- project purpose and endpoints,
- env vars and .env.template usage,
- local dev commands (uvicorn start, docker build & run),
- how to run tests,
- notes on Vertex/Gemini integration and where to insert API calls,
- deployment steps using cloudrun_deploy.sh."

# ------------------------------
# 16 - CI workflow (GitHub Actions) for tests & build
# ------------------------------
Prompt:
"Generate `.github/workflows/ci-backend.yml` that:
- triggers on push to main and PRs,
- sets up python 3.11,
- installs deps, runs pytest,
- builds Docker image, and optionally pushes to GCR if secrets are present,
- fails early on test failures."

# ------------------------------
# 17 - helper prompt: 'generate file set' to create minimal runnable backend
# ------------------------------
Prompt:
"Generate a minimal runnable backend package with the following files assembled: models.py, triage.py, a2a.py, runbook.py (stubbed to return template runbooks), policy.py, simulate.py, db.py (stubbed to use sqlite or in-memory for dev), main.py, Dockerfile, requirements.txt. The produced code must run locally without Vertex/Gemini keys (Gemini calls stubbed to return canned responses if keys missing). Provide instructions to run and a sample curl command to exercise /simulate_flow."

# ------------------------------
# 18 - prompt for iterating & hardening LLM outputs with LangChain + Pydantic
# ------------------------------
Prompt:
"Generate a robust LangChain chain in `backend/app/chains.py` that:
- uses LangChain's Vertex model wrapper to call Gemini,
- uses LangChain output parsers to enforce a JSON schema,
- wraps LLM calls with a try/except and one retry with a corrective prompt if parsing fails,
- returns Pydantic-validated objects from models.py,
- include an example usage snippet that generate_runbook in runbook.py will import."

# ------------------------------
# 19 - prompt for adding A2A message logging to Cloud Logging
# ------------------------------
Prompt:
"Generate code snippet to insert into `a2a.py` or main middleware that logs every A2A JSON message to Cloud Logging (structured JSON). Use google-cloud-logging client to write entries and fallback to stdout if credentials missing. Include sample log fields: trace_id, message_id, actor_from, actor_to, type, payload_summary."

# ------------------------------
# 20 - prompt for endpoint docs & OpenAPI exposure
# ------------------------------
Prompt:
"Generate FastAPI route docstrings and OpenAPI schema enhancements for /triage and /simulate_flow. Include example request/response bodies, response_model references to the Pydantic models, and tags grouping for 'Agent Tools'. Add a short description for each field."
